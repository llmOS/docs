---
title: 'Configuration'
---

There are many things you can optionally configure in OpenCopilot. To change those, pass in the relevant argument to the `OpenCopilot()` initializer.

### 1. Basic Configuration

* `prompt` / `prompt_file`: Define the system prompt. Use `prompt` to pass a string as prompt, or `prompt_file` to specify a path to a prompt file.
* `openai_api_key`:  OpenAI API key. Used for making LLM and embedding calls if chosen LLM is OpenAI.
* `copilot_name`: Name of the copilot. The name will be used to save the unique state of the copilot, for example the cache for downloaded knowledge base files and embeddings. Default: "default".

### 2. Model Settings

* `llm`: Determines what LLM the copilot will use. Can be either an OpenAI LLM name as a string (`gpt-3.5-turbo-16k` or `gpt-4`) or an instance of `langchain.chat_models.base.BaseChatModel`. Default: `gpt-3.5-turbo-16k`.
* `embedding_model`: Determines what embedding model the copilot will use. Can be either an OpenAI embedding model name as a string (like `text-embedding-ada-002`) or an instance of `langchain.embeddings.base.Embeddings`. Default: `text-embedding-ada-002`.
* `max_document_size_mb`:Maximum document size accepted for ingestion, in MB. Default: 50.

### 3. Messaging Templates

When using an OSS LLM the prompt template needs to be provided. For example LLama2 is trained on a specific prompt structure that looks like this:

```
<s>[INST] <<SYS>>
You are a Copilot
<</SYS>>
What are all these weird symbols? [/INST]
```

* `question_template`: A template used to identify and label where a message comes from in a back-and-forth conversation. For example with `llama-2-13b-chat` the template should be `{question} [/INST]`. Default: `### Human: {question}`.
* `response_template`: A template used to identify and label where a message comes from in a back-and-forth conversation. For example with `llama-2-13b-chat` the template should be `{response} </s><s> [INST]`. Default: `### Assistant: {response}`.

### 4. Network Settings

* `host`: Bind socket to this host. Default: `127.0.0.1`.
* `api_port`: Define the socket port. Default: `3000`.

### 5. Environment and Permissions

* `environment`: Specify the environment, e.g., production or development. Default: `local`.
* `allowed_origins`: Set which origins can access the response. Default: `*`.

### 6. Knowledge Base Configuration

* `weaviate_url`: The URL of the Weaviate vector store that will be used for the [Knowledge base](/create/knowledge-base). If no url is provided copilot will start an embedded Weaviate. Default: None.
* `weaviate_read_timeout`: Maximum wait time for a vector store query. Default: 120 seconds.

### 7. Authentication and Security

* `auth_type`: Choose the authentication method for REST API calls. Options: `JWT`, `API_KEY`, or `None`. Depending on your choice, you need to provide additional parameters:
    * For `auth_type="API_KEY"`, provide the `api_key` parameter. This string can then be used to authenticate when calling the REST API.
    * For `JWT` (JSON Web Token) authentication, provide the following parameters:
        * `jwt_client_id`
        * `jwt_client_secret`
        * `jwt_token_expiration_seconds`: specifies how long the JWT token is valid for. Default: 1 day.

### 8. Monitoring and Logging

* `helicone_api_key`: [Helicone](https://www.helicone.ai/) API key. Used for [Monitoring](/integrate/monitoring).
* `helicone_rate_limit_policy`: If Helicone is enabled, implements a rate limiting policy to ensure fair resource usage - see [Helicone rate limiting docs](https://docs.helicone.ai/features/advanced-usage/custom-rate-limits).
* `log_level`: Default Python `logging` module log level. See docs [here](https://docs.python.org/3/library/logging.html#levels)

To see the full and up-to-date list, see the `OpenCopilot` class initializer and method implementations on the [OpenCopilot Github](https://github.com/opencopilotdev/opencopilot/blob/main/opencopilot/application.py).