---
title: "Prompting"
---

<img height="200" src="/images/prompt_meme.png" />


Writing a system prompt is the first big step in creating a custom copilot for your custom use case.
In the system prompt, you instruct your copilot about its goals, behaviour, communication style, etc. 
You can make it follow specific rules on how you would like it to respond to the user.


## Writing a system prompt

Don't worry about getting the prompt perfect on your first try - improving it is an iterative process.

### Things to consider

1. Make sure what you are building a copilot for and how you expect it to behave (see [examples](https://github.com/opencopilotdev/opencopilot/blob/main/examples) to get inspired)
2. Every LLM has a unique purpose and prompting technique (e.g. GPT-4 vs Llama-13B) - we recommend starting with OpenAI GPTs
3. At first, it is easier to start editing an [example](https://github.com/opencopilotdev/opencopilot/blob/main/examples) to match your use case
4. Start off with a simple prompt, then iterate and add instructions to narrow it down to your use case
5. More text doesn't always make it better -> overloading the LLM with prompts can also decrease it's performance

### 1st prompt iteration

Following is a very simple system prompt for an E-Commerce Copilot (good for GPT-3.5 & 4). Something like this is usually more than enough for the first iteration.
You can see it describing:
* who does the copilot portray itself as
* what is the copilot supposed to do

```text
You are an E-Commerce Copilot, an AI assistant for entrepeneurs selling products on their online shops.
You are having a conversation with a user who is an entrepeneur owning an online shop that sells running wear. 

=========
{context}
=========

{history}
User: {question}
E-Commerce Copilot answer in Markdown:
```

### Prompt file structure

The prompt is not a static piece of text. It is a template that is dynamically **filled at runtime**.
This way the copilot is aware of real-time information unique to the interaction, hence you should always place three template variables into the prompt file (ideally at the end),
and they will be substituted as follows:

* `{context}`: the most relevant documents retrieved from your knowledge base (more on this later).
* `{history}`: the conversation history between the user and the copilot.
* `{question}`: the most recent input from the user -- the message they just sent.

**Do not remove or add any variables in prompts, variables that are in curly braces \{\}. (For example: \{context\})**

## Improving the system prompt

There aren't many hard rules to prompting, but making the process iterative and monitoring it's behaviour is necessary to build a good copilot.

### 2nd and 3rd prompt iteration

We'll dive more deeply into debugging later, but nr 1 rule of developing a copilot is actually [chatting](/welcome/getting-started#4-chat-with-the-copilot) with it and seeing if the changes improve it or not.
When you feel it is not quite there yet, it can be narrowed down to act more specifically for your use case by adding to the prompt.
For example:
* what is it's mission
* restrictions on what not to do
* what it can use to reply to the user
* how to act in case of... with concrete examples

```text
You are an E-Commerce Copilot, an AI assistant for entrepeneurs selling products on their online shops.
You are having a conversation with a user who is an entrepeneur owning an online shop that sells running wear. 
Your mission is to help the user build a better business and increase shop's revenue by answering user's questions with advice.
DO NOT provide a generic answer, try to understand user's context and be specific. 
You are given the context to answer each question below.
If the question is too generic, for example "How to make money?" ask user to be more specific to his or her shop.

=========
{context}
=========

{history}
User: {question}
E-Commerce Copilot answer in Markdown:
```
Creating a production-ready system prompt is like fine tuning a model. Depending on your use case it might start off simple but
get more advanced as you develop the copilot, for example see the prompt file for Ready Player Me Copilot in our [examples folder](https://github.com/opencopilotdev/opencopilot/blob/main/examples/ready_player_me_copilot/prompts/prompt_template.txt).

Prompt engineering itself is a deep topic; for a more in-depth overview, see [OpenAI's list of prompting guides](https://github.com/openai/openai-cookbook#prompting-guides). But if you only have time for one tip: in addition to describing the desired behaviour, also give examples.

**The third (and fourth, etc.) iteration could be repeating these simple steps: change prompt -> chat -> improve. Until you get the copilot to behave as you expected. It is also important to add knowledge base, debug retrieval and evaluate copilot end-to-end, which we'll cover as next steps.**

## Using a system prompt

A system prompt can be used as a `String` in the copilot python file or imported as `.txt` file.

### Using system prompt as a String
```python
from opencopilot import OpenCopilot

PROMPT = """
You are a Parrot Copilot. Your purpose is to repeat what the user says, but in a different wording.

=========
{context}
=========

{history}
User: {question}
Parrot Copilot answer in Markdown:
"""

copilot = OpenCopilot(
    openai_api_key="your-openai-api-key",
    llm_model_name="gpt-3.5-turbo-16k",
    prompt=PROMPT
    )

copilot()
```

### Using system prompt from a text file

Simply create a `.txt` containing the prompt text and use it as `prompt_file` to initialize the copilot.
Make sure you do not have both `prompt` and `prompt_file` present in your copilot initialization. 
It would look like somthing this:
```python
copilot = OpenCopilot(
    openai_api_key="your-openai-api-key",
    llm_model_name="gpt-3.5-turbo-16k",
    prompt_file="my_prompt.txt"
    )
```

When changing the system prompt source or the prompt itself in the Python file, re-run the copilot with `python copilot.py`.