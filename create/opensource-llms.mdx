---
title: "[in development] Working with Open Source Large Language Models"
---

OpenCopilot CLI includes a helpful `oss` command that help you choose, setup and run an Open Source LLM locally.

### Prerequisites

OpenCopilot's Open Source LLM support is based on `llama.cpp` therefore it requires `llama-cpp-python` library to work:

```bash
pip install llama-cpp-python
```

More information on how to install can be found at [llama-cpp-python installation docs](https://llama-cpp-python.readthedocs.io/en/latest/#installation).


#### macOS with Apple Sillicon

If you are running OpenCopilot on macOS with Apple Sillicon you can enable Metal support for best performance:
```bash
CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python
```

Check out [llama-cpp-python macOS installation guide](https://llama-cpp-python.readthedocs.io/en/latest/install/macos/) for details.


### Choosing an LLM

To see a list of all the models available:
```bash
# opencopilot oss list
┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┓
┃   ┃ NAME             ┃ SIZE   ┃ INSTALLED ┃
┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━┩
│ * │ Llama-2-7b-chat  │ 3.83GB │ Yes       │
│ * │ Llama-2-13b-chat │ 7.37GB │ Yes       │
│   │ Llama-2-70b-chat │ 38.9GB │ No        │
│ * │ CodeLlama-7b     │ 3.83GB │ No        │
│ * │ CodeLlama-13b    │ 7.37GB │ No        │
│   │ CodeLlama-34b    │ 19.1GB │ No        │
└───┴──────────────────┴────────┴───────────┘

* Recommended for your system

To see more details about a model: opencopilot oss info <model_name>

```
Models marked with an asterisk are recommended ones based on your system performance (RAM, GPU,...)

### LLM model details

To see model details:
```bash
# opencopilot oss info llama-2-13b-chat
Model Name:   Llama-2-13b-chat                                                                                                                                                                                                                                                                                
 Size:         7.37 GB                                                                                                                                                                                                                                                                                         
 Description:  Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. 
```

### Running an LLM

To use a certain LLM:
```bash
# opencopilot oss run llama-2-13b-chat
Downloading Llama-2-13b-chat...
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7.37G/7.37G [00:00<00:00, 33.6kB/s]
Loading Llama-2-13b-chat, please wait...
INFO:     Started server process [62436]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://localhost:8000 (Press CTRL+C to quit)
```

The model will be downloaded upon first use and then stored in the `~/.opencopilot/models` directory.

Now the LLM is waiting for Copilot requests at port 8000. 

A simple Copilot using the local LLM  would then look like this:

```python
from opencopilot import OpenCopilot
from opencopilot.domain.chat.models import LocalLLM
from langchain.embeddings import HuggingFaceEmbeddings

llm = LocalLLM(
    llm_url="http://127.0.0.1:8000/v1",
)

embeddings = HuggingFaceEmbeddings(model_name="thenlper/gte-base")

copilot = OpenCopilot(
    prompt_file="my_prompt.txt",
    llm=llm,
    embedding_model=embeddings,
    )

copilot()
```

Note that to run this example, **you need to also install sentence-transformers** with `pip install sentence_transformers` because we are using a local embeddings model.


### Generating a Model-Specific Prompt Template

Different language models might have different strengths, specializations, or recommended use-cases. 
To generate a prompt template tailored to a specific model, you can use the generate_prompt command followed by the model's name:

```bash
# opencopilot oss prompt llama2-7b-chat
<s><SYS>
Your are a Parrot Copilot. Your purpose is to repeat what the user says, but in a different wording.
</SYS>
<INST>
{context}
Here is the latest conversation between Assistant and User:
{history}
</INST
User: {question}
```

### Removing an LLM

Allows users to remove a model if they no longer need it to save space:
```bash
# opencopilot oss remove llama-2-7b-chat
LLM llama-2-7b-chat removed successfully.
```
