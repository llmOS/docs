---
title: "Working with Open Source Large Language Models"
---
<Warning>Available since version 0.3.6</Warning>

OpenCopilot CLI includes a helpful `oss` command that help you choose, setup and run an Open Source LLM locally.

## Prerequisites

OpenCopilot's Open Source LLM support is based on `llama.cpp` therefore it requires `llama-cpp-python` library to work:

```bash
pip install llama-cpp-python
```

More information on how to install can be found at [llama-cpp-python installation docs](https://llama-cpp-python.readthedocs.io/en/latest/#installation).


#### macOS with Apple Sillicon

If you are running OpenCopilot on macOS with Apple Sillicon you can enable Metal support for best performance:
```bash
CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python
```

Check out [llama-cpp-python macOS installation guide](https://llama-cpp-python.readthedocs.io/en/latest/install/macos/) for details.


## Available LLMs

Here is a comprehensive list of all the open-source large language models supported by OpenCopilot:

| Model Name        | System Requirements      | License | Parameters Count | File Size | Description |
|-------------------|--------------------------|---------|------------------|-----------|-------------|
| Llama-2-7b-chat   | Requires at least 8GB of RAM* | [Free for research and commercial use](https://github.com/facebookresearch/llama/blob/main/LICENSE) | 7 billion | 3.83 GB | Meta developed and publicly released the Llama 2 family of large language models (LLMs) optimized for dialogue use cases. |
| Llama-2-13b-chat  | Requires at least 16GB of RAM* | [Free for research and commercial use](https://github.com/facebookresearch/llama/blob/main/LICENSE) | 13 billion | 7.37 GB | Meta developed and publicly released the Llama 2 family of LLMs optimized for dialogue use cases. |
| Llama-2-70b-chat  | Requires at least 64GB of RAM* | [Free for research and commercial use](https://github.com/facebookresearch/llama/blob/main/LICENSE) | 70 billion | 38.9 GB | Meta developed and publicly released the Llama 2 family of LLMs optimized for dialogue use cases. |
| CodeLlama-7b      | Requires at least 8GB of RAM* | [Free for research and commercial use](https://github.com/facebookresearch/codellama/blob/main/LICENSE) | 7 billion | 3.83 GB | Pretrained and fine-tuned generative text models designed for general code synthesis and understanding. |
| CodeLlama-13b     | Requires at least 16GB of RAM* | [Free for research and commercial use](https://github.com/facebookresearch/codellama/blob/main/LICENSE) | 13 billion | 7.37 GB | Pretrained and fine-tuned generative text models designed for general code synthesis and understanding. |
| CodeLlama-34b     | Requires at least 64GB of RAM* | [Free for research and commercial use](https://github.com/facebookresearch/codellama/blob/main/LICENSE) | 34 billion | 19.1 GB | Pretrained and fine-tuned generative text models designed for general code synthesis and understanding. |

\* Hardware acceleration is highly recommended for optimal performance. Refer to the [LLAMA CPP Python documentation](https://llama-cpp-python.readthedocs.io/en/latest/#installation-with-hardware-acceleration) for guidance on how to enable it.


## Selecting the Right LLM for Your System

The OpenCopilot CLI offers a convenient way to display all available models and highlights those that are best suited for your specific system setup.

To fetch a list of models, simply run:

```bash
# opencopilot oss list
┏━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┓
┃   ┃ NAME             ┃ SIZE   ┃ INSTALLED ┃
┡━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━┩
│ * │ Llama-2-7b-chat  │ 3.83GB │ Yes       │
│ * │ Llama-2-13b-chat │ 7.37GB │ Yes       │
│   │ Llama-2-70b-chat │ 38.9GB │ No        │
│ * │ CodeLlama-7b     │ 3.83GB │ No        │
│ * │ CodeLlama-13b    │ 7.37GB │ No        │
│   │ CodeLlama-34b    │ 19.1GB │ No        │
└───┴──────────────┴───────┴─────────┘
```
Models that have an asterisk (*) next to them are recommended based on your system's capabilities (e.g., RAM, GPU). This helps ensure optimal performance and user experience.

To delve into more specific details about a particular model, use: `opencopilot oss info <model_name>`.

## LLM model details

To see model details:
```bash
# opencopilot oss info llama-2-13b-chat
Model Name:   Llama-2-13b-chat                                                                                                                                                                                                                                                                                
 Size:         7.37 GB                                                                                                                                                                                                                                                                                         
 Description:  Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. 
```

## Running an LLM

To use a certain LLM:
```bash
# opencopilot oss run llama-2-13b-chat
Downloading Llama-2-13b-chat...
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7.37G/7.37G [00:00<00:00, 33.6kB/s]
Loading Llama-2-13b-chat, please wait...
INFO:     Started server process [62436]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://localhost:8000 (Press CTRL+C to quit)
```

The model will be downloaded upon first use and then stored in the `~/.opencopilot/models` directory.

Now the LLM is waiting for Copilot requests at port 8000. 

A simple Copilot using the local LLM  would then look like this:

```python
from opencopilot import OpenCopilot
from opencopilot.domain.chat.models.local import LocalLLM
from langchain.embeddings import HuggingFaceEmbeddings

llm = LocalLLM(
    llm_url="http://127.0.0.1:8000/",
)

embeddings = HuggingFaceEmbeddings(model_name="thenlper/gte-base")

copilot = OpenCopilot(
    prompt_file="my_prompt.txt",
    question_template=" {question} [/INST] ",
    response_template="{response} </s><s> [INST]",
    llm=llm,
    embedding_model=embeddings,
    )

copilot()
```

Note that to run this example, **you need to also install sentence-transformers** with `pip install sentence_transformers` because we are using a local embeddings model.


## Generating a Model-Specific Prompt Template

Different language models might have different strengths, specializations, or recommended use-cases. 
To generate a prompt template tailored to a specific model, you can use the generate_prompt command followed by the model's name:

```bash
# opencopilot oss prompt llama2-7b-chat
<s>[INST] <<SYS>>
Your purpose is to repeat what the user says, but in a different wording.
Don't add anything, don't answer any questions, don't give any advice or comment - just repeat.
Context:
{context}
<</SYS>>

{history} Repeat: {question} [/INST]
```

## Removing an LLM

Allows users to remove a model if they no longer need it to save space:
```bash
# opencopilot oss remove llama-2-7b-chat
LLM llama-2-7b-chat removed successfully.
```
