---
title: "Stack"
description: "The stack is simple to use, iterate and develop further on for production use"
---

1. 🧠 LLM: OpenAI GPT-4 by default; any LLM can be used
1. 🤖 OpenCopilot back-end
    1. Python & [FastAPI](https://fastapi.tiangolo.com/) server
    1. 🔗 [Langchain](https://github.com/hwchase17/langchain) for prompting and chaining
    1. 📃 Weaviate for memory and retrieval
    1. 📚 Simple to use data ingestion
1. 🖼️ Front-end: Next.js and [Vercel AI SDK](https://sdk.vercel.ai/docs)
1. 📊 Analytics: [Helicone](https://www.helicone.ai/) (optional)

At the core of the OpenCopilot stack is the OpenCopilot Server. The server sits on top of the LLM but under your application, and implements the logic of the copilot: ingesting data, retrieving context at runtime, prompting, and parsing the results.

You can use any LLM and embedding model with the OpenCopilot Server, and any front-end. We provide default templates for both for a quicker start.

<img src="https://i.imgur.com/qJuQUj5.png" alt="Stack diagram of OpenCopilot" width="70%" noZoom />
