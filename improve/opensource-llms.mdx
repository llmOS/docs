---
title: "Working with Open Source Large Language Models"
---

OpenCopilot CLI includes a helpful `oss_model` command that help you choose, setup and run an Open Source LLM locally.

### Choosing an LLM

To see a list of all the models available:
```bash
# opencopilot oss_model list
    NAME                      	SIZE  
 *  llama2-7b-chat              3.8 GB
    openorca-platypus2-13b.    	7.3 GB
* Recommended for your system

```
Models marked with an asterisk are recommended ones based on your system performance (RAM, GPU,...)

### Using an LLM

To use a certain LLM:
```bash
# opencopilot oss_model use llama2-7b-chat
Downloading llama2-7b-chat...
Starting up...
LLM started, please check that your Copilot matches contains these settings:
     ...
     llm_url="http:/127.0.0.1:3030"
     ...
```

The a simple `copilot.py` file would then look like this:

```python
from opencopilot import OpenCopilot

copilot = OpenCopilot(
    prompt_file="my_prompt.txt",
    llm_url="http:/127.0.0.1:3030"
    )

copilot()
```

