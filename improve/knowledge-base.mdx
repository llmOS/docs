---
title: "Knowledge base"
---

The **knowledge base** is the set of documents that your copilot relies on when responding - you can think of it as the things you've taught it. The copilot has access to everything here when trying to answer your questions.

### Prerequisites

To use the knowledge base, you need to set up a Weaviate vector store.

Make sure you have [Docker installed](https://docs.docker.com/engine/install/) and then run:

```shell
docker run \
  -e CONTEXTIONARY_URL="contextionary:9999" \
  -e QUERY_DEFAULTS_LIMIT="25" \
  -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED="true" \
  -e PERSISTENCE_DATA_PATH="/var/lib/weaviate" \
  -e CLUSTER_HOSTNAME="node1" \
  -p 8080:8080 \
  semitechnologies/weaviate:1.19.6 \
    --host 0.0.0.0 --port '8080' --scheme http
```


### Adding files and folders to the knowledge base

To add new knowledge and your own data to the Copilot simply let the copilot know which files to load.

```python
# Loads the entire folder and subfolders
copilot.add_local_files("mydata/")

# Loads just one file
copilot.add_local_file("mydata/article.pdf")
```

The following file formats are supported by default:

* `pdf` files
* `csv`, `tsv` and `xls`/`xlsx` (Excel) files
* `txt` files
* `json` files

### Adding a custom document loader

In some situations, our default file loaders may not be sufficient. For example:

* You have a custom file format that you want to load into the knowledge base.
* Your files are large and you want to have control over how they are chunked.
* Instead of the local filesystem, you want to load files from a remote location like Amazon S3, a URL, a database, etc.

In these situations, you can write a custom document loader. A document loader is simply a function that returns a list of documents. To add a custom document loader, decorate it with `@copilot.data_loader`:

```python
from typing import List
from langchain.schema import Document

@copilot.data_loader
def custom_dataloader() -> List[Document]:
    pass # This is where you fetch and return your documents
```

Each document you return should be of the type [`langchain.schema.Document`](https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/schema/document.py). This is a simple dataclass that contains two fields: `page_content` is the content of the document as a string, which will be indexed for search and retrieved into the prompt at runtime. `metadata` is a dictionary of arbitrary metadata about the document, which will be stored in the knowledge base but not indexed for search; the most important keys in that dictionary are `source` and `title` which, when set, can appear in the copilot's responses.

For example, to retrieve one URL and load it into the knowledge base, this is what your `copilot.py` file would look like:

```python
import requests
from typing import List
from langchain.schema import Document
from opencopilot import OpenCopilot

copilot = OpenCopilot()

copilot.add_prompt("myprompt.txt")

@copilot.data_loader
def custom_dataloader() -> List[Document]:
    url = "https://www.example.com/article.html"
    response = requests.get(url)
    return [Document(page_content=response.text, metadata={"source": url})]

copilot()
```

You can also use any existing [Langchain document loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/). For example, using the CSV loader and the Notion loader:

```python
from langchain.document_loaders.csv_loader import CSVLoader

@copilot.data_loader
def load_csv_data() -> List[Document]:
    loader = CSVLoader(file_path='./example_data/mlb_teams_2012.csv')
    documents = loader.load()
    return documents


from langchain.document_loaders import NotionDirectoryLoader

@copilot.data_loader
def load_notion_data() -> List[Document]:
    dump_path = "./my_notion_dump"
    loader = NotionDirectoryLoader(dump_path)
    documents = loader.load()
    return documents
```

To ingest new data, the copilot needs to be restarted by killing and re-running the Python app with `python copilot.py`.

### Caching

By default, every ingested document in the knowledge base is cached locally. The cache operates on the basis of the `Document`'s content (i.e. the `page_content` field): if the hash of the content is the same, the embedding is retrieved from the cache. Otherwise the document is re-embedded and the cache is updated.